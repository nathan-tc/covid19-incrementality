import numpy as np
import pandas as pd
import argparse
from pathlib import Path
import os
from zipfile import ZipFile

age_intervals = [0,19,39,59,74,200]
age_bin_labels = ['00 à 19 ans',
                  '20 à 39 ans',
                  '40 à 59 ans',
                  '60 à 74 ans',
                  '75 ans et plus']
dtypes = {'annee_comptabilisation': 'int64',
          'sexe': 'int64',
          'age': 'int64', 
          'departement_deces': 'str', 
          'nb_deces': 'int64', 
          'date_deces': 'str'}


def extract_archives(filepath, outputdir='.temp/'):
    """extract zip file to temp folder

    Parameters
    ----------
    filename : str
        name of the .zip file to preprocess including the extension (e.g. ".data/test.zip").
    outputdir: str (default: "./temp/")
        folder path for the output extracted file
    """

    # Create a ZipFile Object and load sample.zip in it
    with ZipFile(filepath, 'r') as zipObj:
        # Extract all the contents of zip file in different directory
        zipObj.extractall(path=outputdir)

def load_preprocess_df(df_filename):
    df = pd.read_csv(df_filename, sep=';', dtype=dtypes)
    df['date_deces'] = pd.to_datetime(df['date_deces'])
    df = df[df['date_deces'] >= '1990-01-01']
    # To take age into account, we will create a new feature `age_bin` based on INSEE standard: (0:19, 20:39, 40:59, 60:74, 75+)
    df['age_bin'] = pd.cut(df['age'], age_intervals, include_lowest=True, labels=age_bin_labels)
    # Aggregate by age_bin rather than age
    df = df.groupby(["date_deces", "age_bin", "sexe", "departement_deces"], as_index=False)["nb_deces"].sum()
    df['nb_deces'] = df['nb_deces'].astype(pd.Int64Dtype())  # pandas int that handle NaN...
    df['departement_deces'] = df['departement_deces'].astype('category')
    return df

def add_geographical_info(df):
    departements = pd.read_csv('./data/departements.csv', sep=';')
    df = df.join(departements[['departement_code','departement','region']].set_index('departement_code'), on='departement_deces') 
    return(df)

def add_population_info(df):
    pop_dep = pd.read_csv("./data/population_insee_dept_year_sex_age.csv", sep=';')
    pop_dep = pop_dep[['annee','departement_code','sexe_code','classe_age','population']]
    pop_dep.columns = ['year','departement_code','sexe','age_bin','dep_pop']
    df['year'] = df['date_deces'].dt.year
    df = df.merge(pop_dep, 
                  left_on=['year','departement_deces','sexe','age_bin'], 
                  right_on=['year','departement_code','sexe','age_bin'])
    df = df.drop('year', 1)
    return(df)

def add_datetime_features(df):
    df['year'] = df['date_deces'].dt.year
    df['month'] = df['date_deces'].dt.month
    df['weeknumber'] = df['date_deces'].dt.week
    df['dayofweek'] = df['date_deces'].dt.dayofweek
    df['day'] = df['date_deces'].dt.day
    return(df)

def preprocess_data(filename,
                    inputdir="./data/",
                    outputdir="./preprocessed_data/"):
    """preprocess a file from ./data/ and save it as csv gzipped in  ./preprocessed_data/

    Parameters
    ----------
    filename : str
        name of the .csv file to preprocess without the extension (e.g. "test" for file `test.csv`).
    inputdir : str (default: "./data/")
        folder path containing the file
    outputdir: str (default: "./preprocessed_data/")
`       folder path for the output preprocessed file

    Returns
    -------
    output_filepath: str
        the signal
    """

    Path(outputdir).mkdir(parents=True, exist_ok=True)

    print(f"Load and process {filename}")
    df = load_preprocess_df(os.path.join(inputdir, filename+'.csv'))
    print(f"Add departement and region names")
    df = add_geographical_info(df)
    
    output_filepath = os.path.join(outputdir, filename+'.csv.gz')
    print(f"Save {output_filepath}")
    df.to_csv(output_filepath, index=False, compression='gzip')

    return output_filepath

def add_datetime_and_groupby_week(df):
    df['date_deces'] = pd.to_datetime(df['date_deces'])
    df['year'] = df['date_deces'].dt.year
    df['weeknumber'] = df['date_deces'].dt.week
    df = df.groupby(["year", "weeknumber", "age_bin", "sexe", "departement", "region"], as_index=False)["nb_deces"].sum()
    return(df)

def historical_data_as_weekly(outputdir="./preprocessed_data/"):
    '''
    Input: takes preprocessed CSVs for each decade (as generated by preprocess_data()) 
    Output (save) one csv grouped by weeknumber rather than exact date.
    
    '''
    filename = 'INSEE_deces_1990_1999'
    print(f"Aggregate by week and save {filename}")
    df1 = pd.read_csv("./preprocessed_data/"+filename+".csv.gz", compression='gzip')
    df1 = add_datetime_and_groupby_week(df1)
    df1.to_csv(os.path.join(outputdir, filename+'.csv.gz'), index=False, compression='gzip')

    filename = 'INSEE_deces_2000_2009'
    print(f"Aggregate by week and save {filename}")
    df2 = pd.read_csv("./preprocessed_data/"+filename+".csv.gz", compression='gzip')
    df2 = add_datetime_and_groupby_week(df2)
    df2.to_csv(os.path.join(outputdir, filename+'.csv.gz'), index=False, compression='gzip')

    filename = 'INSEE_deces_2010_2019'
    print(f"Aggregate by week and save {filename}")
    df3 = pd.read_csv("./preprocessed_data/"+filename+".csv.gz", compression='gzip')
    df3 = add_datetime_and_groupby_week(df3)
    df3.to_csv(os.path.join(outputdir, filename+'.csv.gz'), index=False, compression='gzip')
    
    print(f"Concatenate all historical files")
    df = pd.concat([df1,df2,df3])

    # Since these records can contain old data in new records (e.g. new death from 1992 added in a 2010 file)
    # We redo a groupby here with sum to add all these up
    df = df.groupby(["year", "weeknumber", "age_bin", "sexe", "departement", "region"], as_index=False)["nb_deces"].sum()
    
    filename = 'INSEE_deces_1990_2019_byweek'
    output_filepath = os.path.join(outputdir, filename+'.csv.gz')
    print(f"Save {output_filepath}")
    df.to_csv(output_filepath, index=False, compression='gzip')